---
layout: page
title: "Intermediate Data Science (DS 100)"
description: "Intermediate Data-Science at Berkeley"
<!-- group: navigation -->
order: 4
use_math: true
---
{% include JB/setup %}

This document contains early descriptions of a course that is actively under development.
I am sharing this information to collect feedback and help students plan for future courses.
Unfortunately, or perhaps fortunately, these plans may change and so students who are interested in DS100 and would like more details should contact [me](index)!

<br/>
<br/>

# DS100: Principles & Techniques of Data Science

We are in the process of developing a new intermediate data science class, tentatively called DS100, that will cover the data science life-cycle spanning data acquisition, cleaning, exploration, summarization, hypothesis testing, and prediction.
Through this process we will explore some of the guiding principles in data management, visualization, statistics, and machine learning.
We will also learn some of the tools and techniques commonly used today.

## Goals

Students completing the course should be able to:

1. begin careers as a data scientist. **Employed**

1. use data and computational thinking to address problems in their lives and careers. **Empowered**

1. more easily master advanced Berkeley courses in data-management ([CS186](http://www.cs186berkeley.net)), machine learning ([CS189](https://people.eecs.berkeley.edu/~jrs/189/)), and statistics (Stat134). **Prepared**



## Prerequisites

While we are working to make this class widely accessible in the initial ([beta](https://en.wikipedia.org/wiki/Software_release_life_cycle)) version of the class we plan to require the following (or equivalent):

1. **(Strongly Encouraged)** *Foundations of Data Science* ([Data 8](http://data8.org/fa16/)).  This is an *excellent* class that covers much of the material in DS100 but at a higher level.  This class provides basic exposure to python programming and working with tabular data as well as visualization, statistics, and ML.

1. **(Math)** *Calculus and Linear Algebra* ([Math 54](https://math.berkeley.edu/~nadler/54fall2015.html) or [EE 16](http://inst.eecs.berkeley.edu/~ee16a/fa16/)): We will need some basic concepts like linear operators, eigenvectors, derivatives, and integrals to enable statistical inference and derive new prediction algorithms.  Students with a strong mathematics background may be able to skip this prerequisite.

1. **(Computing)** *Algorithms and Data-Structures* ([CS61b](http://datastructur.es/sp16/)): Students will need a basic understanding of iteration, recursion, data-structures, and computational complexity.   However students with strong backgrounds in computing may be able to skip this prerequisite.


## Current *Tentative* Course Outline

{% assign counter = 1 %}

### Module 0: *Course Overview*

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Course Overview*:** What is data science and how does this class relate to it?

{% include collapse begin="true" %}
This lecture will setup the importance of data centric thinking by discussing how data is shaping society and how data science is quickly becoming a driving force across government, industry, and research.  We will introduce the data science life-cycle and describe the skills required in data science. Finally, we will provide background on the teach staff and organization of the class.

#### Learning Objectives:
* The importance of data and computational thinking in society
* The roles and skills of data scientists
* Organization of the class

{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *The Data Science Lifecycle*:** What is the workflow of a data scientist and what should I have learned in DS8?

{% include collapse begin="true" %}
This lecture will work through each state of the data science life-cycle by examining a real dataset.  This lecture we will refresh the students on the DS8 dataframe and the notebook interface.  The lecture will span data acquisition, some basic cleaning, visualization, statistical summarization, and ultimately we will build a basic predictive model.

#### Learning Objectives:
* The stages of the data science life-cycle
* Recall how to work with dataframes and notebooks
* Concepts covered throughout the rest of the class

{% include collapse end="true" %}


### Module 1: *Kinds of Data*

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Working with Text and Semi-Structured Data*:** How do I transform textual and semi-structured data into easy to manipulate tabular representations?

{% include collapse begin="true" %}
In this lecture we will begin to get our hands dirty with real data.  In particular we will work with CSV and JSON log data and discuss how to transform this data into a table.

#### Learning Objectives:
* Become familiar with the challenges of operating on text and semi-structured data
* Learn basics of regular expressions
* Learn how to manipulate and traverse DOM/JSON

{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Spatial and Temporal Data*:** How do I reconcile time and space across different datasets.

{% include collapse begin="true" %}
This lecture will discuss the ubiquity of spatial and temporal data and discuss some of the key questions and challenges surrounding this data.  In particular, where and why do we study spatial and temporal data, how do we align space and time across datasets, the need for smoothing, interpolation, and extrapolation, as well as the various representations of space and time.
{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Multimedia Data*:** How do I extract and encode information from images and videos?

{% include collapse begin="true" %}
This lecture will deal with the various formats of multimedia data and the challenges of working with this data.  We will discuss issues around meta-data as well as the use of techniques in computer vision to extract important signal from this data that we can later analyze.  This will bring some discussion about machine learning earlier into the class.
{% include collapse end="true" %}

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Network Data*:** How do I reason about relationships between data?

{% include collapse begin="true" %}
This lecture will discuss the use of graphs as a mechanism to represent data.  We will review the various types of graphs (directed, undirected, bipartite, DAGs) and their interpretations as data.  We will discuss how graphs can be stored in dataframes and some of the important questions we will want to answer about graphs as we move through the course.
{% include collapse end="true" %}


### Module 2: *SQL and Data Management*

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Introduction to SQL*:**
   What and why is SQL and how do I use it? (Basic queries)

{% include collapse begin="true" %}
This lecture will introduce the SQL language by motivating the use of a common declarative specification for data transformations.  We will introduce the concept of a database as a system to store, manage, and retrieve data.  We will describe the concept of a table with a schema and introduce basic queries to access the data (select where and groupby).  In this lecture we will assume that the schema and data have already been determined.
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Advanced SQL*:** How do I bring tables together, build complex queries, and reason about their costs?

{% include collapse begin="true" %}
This lecture picks up where the previous left off by introducing joins and more complex nested SQL queries.
{% include collapse end="true" %}

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Data Modeling and Normalization*:** How do I organized for easy access and error free updates.

{% include collapse begin="true" %}
This lecture will cover schema design by introducing the concept of normalization.  We will organize data for one of our running examples.  We will introduce the star-schema as a common mechanism to organize information in a normalized form. Finally we will discuss how to insert and update tuples in a database.
{% include collapse end="true" %}

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Physical Data Management*:** How do I organize data for fast access at scale?

{% include collapse begin="true" %}
This lecture will cover techniques to improve data access performance in database systems.  In particular we will discuss how indexes can be used to improve point lookups and range queries and some of the tradeoffs of adding indexes.   We will also introduce distributed data-processing in the BSP/Map-Reduce paradigm and discuss data partitioning.
{% include collapse end="true" %}

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Big Data Ecosystem*:** How is data stored and managed in large organizations?

{% include collapse begin="true" %}
This lecture will provide a survey of the terms and technologies used to manage data in large organizations.  In particular we will discuss the differences between Data Warehouses, Datamarts and Data Lakes.  We will discuss the disaggregation of data processing systems across HDFS -- (Hadoop, Spark) -- (HIVE, PIG, SparkSQL, Dataframes).  We will also mention some of the non-relational technologies including the common KV-Stores: Casandra and Mongo as well as the Pub-sub/stream systems like Kafka and Storm.  The goal is not to understand the details behind how these technologies work but instead their motivations and the implications on getting data from them.
{% include collapse end="true" %}


### Module 3: *Understanding the Data*

* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Summary Tables and Plots*:** What does the data look like?

{% include collapse begin="true" %}
**To be finished soon!**

* Use of aggregation, rollup, and slicing as mechanism to understand data
* Scatter plots, histograms, whisker plots
* Log scales
* Summarizing text and image data
{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Sketching and Stream Summarization*:** How can I summarize data efficiently at scale?

{% include collapse begin="true" %}
**To be finished soon!**

* Challenges of working with streams of data: limited memory
* Bloom Filters
* Count-Min Sketch
* Reservoir Sampling
{% include collapse end="true" %}




* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Working with Random Variables*:** What do I expect to see?

{% include collapse begin="true" %}
**To be finished soon!**

* Compute expectations
* Manipulate expectations
* Compute marginals and conditionals
* Concept of independence
{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Power Laws*:** What if my data is far from normal?

{% include collapse begin="true" %}
**To be finished soon!**

* Common pattern in many real-world datasets
* Why so common
* How to visualize
* Forms of power-laws
* Implications on statistics
{% include collapse end="true" %}




* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Parameter Estimation*:**How do I estimate the parameters of data distributions?

{% include collapse begin="true" %}
**To be finished soon!**

* Concept of maximum likelihood
* Derivation of maximum likelihood estimators
* Bias and Variance
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Bayesian Posteriors*:** How can I encode prior knowledge into my estimates?

{% include collapse begin="true" %}
**To be finished soon!**

* Motivation and role of priors
* Bayes Theorem
* MAP estimation
* Philosophical discussion about differences in perspectives between Bayesians and frequentists
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Confidence and Credibility Intervals*:** What is a reasonable range of values I might expect?

{% include collapse begin="true" %}
**To be finished soon!**

* Meaning of intervals
* How to compute from data
* Implications of frequentist vs Bayesian

{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Hypothesis Testing*:** Did my treatment have a significant effect?

{% include collapse begin="true" %}
**To be finished soon!**

* Experimental design
* A-B testing (Welch's T-test)
* Likelihood ratio tests
* Bootstrap/Permutation Tests
{% include collapse end="true" %}




### Module 4: *Making Predictions*


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Overview of Machine Learning*:** What is machine learning?

{% include collapse begin="true" %}
**To be finished soon!**

* Applications of ML
* Taxonomy of ML (from 186 lecture)
* Fundamental challenges: noise, signal, objective
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Linear Regression*:** How do I model linear relationships?

{% include collapse begin="true" %}
**To be finished soon!**

* Linear relationships in data
* Likelihood/Loss Functions
* MLE estimation
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Logistic Regression*:** How do I make predictions on categorical data?

{% include collapse begin="true" %}
**To be finished soon!**

* Logistic Likelihood/loss function
* Gradient descent
* Stochastic gradient descent as an estimator of gradient
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Feature Engineering*:** What if there is a non-linear relationship between $X$ and $Y$?

{% include collapse begin="true" %}
**To be finished soon!**

* Categorical data: one-hot-encoding
* Text data: Bag-of-words
* Image data: histograms, filters
* Basis functions
* Allude to over-fitting
{% include collapse end="true" %}




* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Overfitting and the Bias Variance Tradeoff*:** Why don't my predictions generalize?

{% include collapse begin="true" %}
**To be finished soon!**

* Bias / Variance tradeoff at a high-level
* Derive fundamental bias / variance tradeoff for squared loss
* Introduce regularization and connect to priors
* Cross validation algorithm

{% include collapse end="true" %}




* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Classification and Regression Trees*:**  How do I model complex discontinuous relationships?

{% include collapse begin="true" %}
**To be finished soon!**

* Ability to capture piecewise non-linearity
* Basic algorithm (C4.5)
* Challenges of over-fitting pruning and model averaging

{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Advanced Techniques in Supervised Learning*:** To infinite dimensions and beyond.

{% include collapse begin="true" %}
**To be finished soon!**

* The kernel trick as a generalization of feature engineering  SVMs
   * Demonstrate how to use library and discuss tradeoffs
* Deep Learning as a generalization of LR and feature engineering
   * Demonstrate how to use library and discuss tradeoffs
* Ensemble methods and model averaging (connect back to boostrap)
{% include collapse end="true" %}


* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Clustering for unsupervised Learning*:** How do I find groups of similar data.

{% include collapse begin="true" %}
**To be finished soon!**

* Problem formulation
   * Important discussion on distance functions ...
* K-Means clustering algorithm
* Choosing a good initialization
* Distance based clustering (DBSCAN)
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Dimensionality Reduction*:** How do I project my data into a lower dimensional space.

{% include collapse begin="true" %}
**To be finished soon!**

* Curse of dimensionality: challenges of working with high-dimensional data
* Simple techniques to reduce dimension: random projections and hashing
* SVD/PCA as more advanced technique
{% include collapse end="true" %}



* **(Lecture {{ counter }}) {% assign counter = counter | plus: 1 %} *Course Review*:** What did I learn in this class?

{% include collapse begin="true" %}
**To be finished soon!**

* Far too much!
{% include collapse end="true" %}





